{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LpnC1xcsiBie"
   },
   "source": [
    "# Advance Machine Learning Project  Atari Reinforcement Learning with DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8t7JiKFlwQC"
   },
   "source": [
    "# Libraries and Dependencies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qeb1eCd2cS9M"
   },
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nHLkEhOTg1_6"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "import base64\n",
    "import glob\n",
    "import gzip\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gymlogger.set_level(40)  # error only\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2ixO9yd6Yzv"
   },
   "source": [
    "### External Dependencies: Google (Colab Drive), Tensorflow GPU and Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751
    },
    "colab_type": "code",
    "id": "fVAbPmel6cGE",
    "outputId": "1d7c2316-e866-472a-e5d7-60206e95474b"
   },
   "outputs": [],
   "source": [
    "IS_GOOGLE = False\n",
    "SAVE_DIRECTORY_PATH = '/content/drive/My Drive/Colab Notebooks/aml_models/' if IS_GOOGLE else './aml_models/'\n",
    "if IS_GOOGLE:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Install rendering libraries \n",
    "    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "    \n",
    "    from IPython import display as ipythondisplay\n",
    "    from IPython.display import HTML, clear_output\n",
    "    from pyvirtualdisplay import Display\n",
    "\n",
    "    # Install TF 2 and enable GPU\n",
    "    if \"2.\" not in tf.__version__ or not tf.test.is_gpu_available():\n",
    "        !pip uninstall tensorflow\n",
    "        !pip install tensorflow-gpu\n",
    "        print(f\"Python version: {sys.version}\")\n",
    "        print(f\"Tensorflow version: {tf.__version__}\")\n",
    "\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "    # Create virtual display to send rendered frames to in Colab \n",
    "    display = Display(visible=0, size=(1400, 900))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_zHbvd6hwjc"
   },
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9jT9edgDlstf"
   },
   "source": [
    "### Timer Utility \n",
    "Response for timing model operations during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mvHpiv17uN0T"
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, tag):\n",
    "        self.start_time = time.time()\n",
    "        self.tag = tag\n",
    "\n",
    "    def stop(self):\n",
    "        end_time = time.time()\n",
    "        print(f'{self.tag} - Execution time: {round(end_time - self.start_time)} seconds')\n",
    "\n",
    "    def start(self, tag):\n",
    "        self.tag = tag\n",
    "        self.start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cshzld81jINi"
   },
   "source": [
    "### Filehandler\n",
    "Responsible for loading and restoring models during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67APzv5ihueX"
   },
   "outputs": [],
   "source": [
    "class FileHandler:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.timer = Timer(tag=\"\")\n",
    "        self.session_path = f'{SAVE_DIRECTORY_PATH}{self.filename}'\n",
    "        self.memory_path = f'{SAVE_DIRECTORY_PATH}{self.filename}_memory'\n",
    "\n",
    "    def __save(self, session, replay_memory):\n",
    "        print(f'Saving session to {self.session_path}')\n",
    "        self.__save_session_gzip(session, self.session_path)\n",
    "        print(f'Saving memory to {self.memory_path}')\n",
    "        self.__save_memory_gzip(replay_memory,self.memory_path)\n",
    "\n",
    "    def __load(self):\n",
    "        print(f'Fetching session from {self.session_path}')\n",
    "        session = self.__load_session_gzip(self.session_path)\n",
    "        print(f'Fetching memory from {self.memory_path}')\n",
    "        memory = self.__load_memory_gzip(self.memory_path)\n",
    "        return session, memory\n",
    "\n",
    "    def __save_session_gzip(self, session, path):\n",
    "        data = json.dumps(session, separators=(',', ':')) # compression level 0-9 where 9 is highest\n",
    "        with gzip.open(f'{path}.gz', 'wt', encoding=\"ascii\", compresslevel=9) as file:  \n",
    "            file.write(data)\n",
    "        with gzip.open(f'{path}_backup.gz', 'wt', encoding=\"ascii\", compresslevel=9) as file:  \n",
    "            file.write(data)\n",
    "\n",
    "    def __load_session_gzip(self, path):\n",
    "        try:\n",
    "            with gzip.open(f'{path}.gz', 'rt', encoding='ascii') as json_file:\n",
    "                return json.load(json_file)\n",
    "        except:\n",
    "            with gzip.open(f'{path}_backup.gz', 'rt', encoding='ascii') as json_file:\n",
    "                return json.load(json_file)\n",
    "\n",
    "    def __save_memory_gzip(self,memory,path):\n",
    "        with gzip.open(f'{path}.gz','wb') as memory_file:\n",
    "            np.save(file=memory_file, arr = memory )\n",
    "        with gzip.open(f'{path}_backup.gz','wb') as memory_file:\n",
    "            np.save(file=memory_file, arr = memory )\n",
    "\n",
    "    def __load_memory_gzip(self,path):\n",
    "        try:\n",
    "            exists = os.path.exists(f'{path}.gz')\n",
    "            if not exists: return []\n",
    "            \n",
    "            with gzip.open(f'{path}.gz','rb') as memory_file:\n",
    "                return np.load(memory_file, allow_pickle=True)\n",
    "        except:\n",
    "            with gzip.open(f'{path}_backup.gz','rb') as memory_file:\n",
    "                return np.load(memory_file,  allow_pickle=True)\n",
    "    \n",
    "\n",
    "    def check_existing_session(self):\n",
    "        exists = os.path.exists(f'{SAVE_DIRECTORY_PATH}{self.filename}.gz')\n",
    "        if not exists:\n",
    "            print(f\"No session found with given filename {self.filename}. Continuing\")\n",
    "            return None, None\n",
    "\n",
    "        response = input(f'{self.filename} session found. Do you want to restore the model? [y/n]')\n",
    "        if response == 'y':\n",
    "            return self.__load()\n",
    "        else:\n",
    "            raise (BaseException('Cancelled execution - Please use another filename.'))\n",
    "\n",
    "    def save_session(self, agent, environment, episode, average_losses, total_rewards, average_rewards):\n",
    "        print(f'Saving session...')\n",
    "        self.timer.start('Model parameters')\n",
    "        session = {'date': datetime.now().strftime('%Y-%m-%d-%H:%M:%S'), 'episode': episode}\n",
    "        print(session)\n",
    "        session['average_losses'] = average_losses\n",
    "        session['total_rewards'] = total_rewards\n",
    "        session['average_rewards'] = average_rewards\n",
    "        session['agent_params'] = agent.extract_agent_parameters()\n",
    "        session['environment_params'] = environment.extract_environment_parameters()\n",
    "        print('Extracting agent memory...')\n",
    "        replay_memory = agent.extract_replay_memory()\n",
    "        print('Extracting model parameters...')\n",
    "        session['model_params'] = agent.extract_model()\n",
    "        self.timer.stop()\n",
    "        print('Extracted model parameters.')\n",
    "        print('Compressing and storing...')\n",
    "        self.timer.start('Model file storage')\n",
    "        self.__save(session,replay_memory)\n",
    "        self.timer.stop()\n",
    "        print(f'Session saved.')\n",
    "\n",
    "    def load_session(self, agent, environment):\n",
    "        episodes, average_losses, total_rewards, average_rewards = 0, [], [], []\n",
    "        self.timer.start('File fetching.')\n",
    "        session, replay_memory = self.check_existing_session()\n",
    "        if session is None:\n",
    "            self.timer.stop()\n",
    "            return episodes, average_losses, total_rewards, average_rewards\n",
    "        print('Session Fetched and decompressed - Loading Parameters...')\n",
    "        self.timer.start('Load Parameters')\n",
    "        session_date = session['date']\n",
    "        episodes = session['episode']\n",
    "        average_losses = session['average_losses']\n",
    "        total_rewards = session['total_rewards']\n",
    "        average_rewards = session['average_rewards']\n",
    "        print(f'Session Meta - Save Date: {session_date} - episodes: {episodes}')\n",
    "        print('Loading Agent Parameters...')\n",
    "        agent.load_agent_parameters(session['agent_params'])\n",
    "        print('Loading agent memory...')\n",
    "        agent.load_replay_memory(replay_memory)\n",
    "        print('Loading Environment Parameters...')\n",
    "        environment.load_environment_parameters(session['environment_params'])\n",
    "        print('Loading Model Parameters...')\n",
    "        agent.load_model(session['model_params'])\n",
    "        self.timer.stop()\n",
    "        print(f'Loaded session from {self.filename} - Date: {session_date}.')\n",
    "        print(f'Continuing from episode {episodes}')\n",
    "        return episodes, average_losses, total_rewards, average_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ACe0eqe3jTAw"
   },
   "source": [
    "### Video Utility\n",
    "Generates and handles gameplay videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJyHaa1ijAEZ"
   },
   "outputs": [],
   "source": [
    "class VideoUtility:\n",
    "    \"\"\"\n",
    "    Utility functions used to generate video clips of Atari gameplay by the agent.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def wrap_environment(self, env):\n",
    "        return Monitor(env, f\"{SAVE_DIRECTORY_PATH}{self.model_name}_video\", force=True)\n",
    "\n",
    "    def clear_files(self):\n",
    "        path = f'{SAVE_DIRECTORY_PATH}{self.model_name}_video/'\n",
    "        if os.path.exists(path):\n",
    "            json_files = glob.glob(f'{path}*.json')\n",
    "            video_files = glob.glob(f'{path}*.mp4')\n",
    "            if len(video_files) == 1:\n",
    "                return\n",
    "            for f in json_files:\n",
    "                os.remove(f)\n",
    "\n",
    "            for idx, f in enumerate(video_files):\n",
    "                if idx < len(video_files) - 1:\n",
    "                    os.remove(f)\n",
    "\n",
    "    def show_video(self):\n",
    "        mp4list = glob.glob(f'{SAVE_DIRECTORY_PATH}{self.model_name}_video/*.mp4')\n",
    "        if len(mp4list) > 0:\n",
    "            mp4 = mp4list[0]\n",
    "            video = io.open(mp4, 'r+b').read()\n",
    "            encoded = base64.b64encode(video)\n",
    "            ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                        loop controls style=\"height: 400px;\">\n",
    "                        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                    </video>'''.format(encoded.decode('ascii'))))\n",
    "        else:\n",
    "            print(\"Could not find video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B4EG4EPVp20p"
   },
   "source": [
    "### Plot Method \n",
    "Plot method is used to graph rewards and loss over time during training episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9nffu7l5qBAb"
   },
   "outputs": [],
   "source": [
    "def plot(episodes, y, title, ylabel, xlabel=\"Episode (one game until terminal)\"):\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.plot(episodes, y)\n",
    "    plt.grid()\n",
    "    plt.savefig(f'{SAVE_DIRECTORY_PATH}{title}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6aDsGfTDh2nT"
   },
   "source": [
    "# Main Project\n",
    "Implementation of Deep Q-learning Agent (DQN) using deep Convolutional Neural Network and model-free Q Reinforcement Learning  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BP7mJIfOiV6n"
   },
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZNmy3Q1Mg5N9"
   },
   "outputs": [],
   "source": [
    "MINIBATCH_SIZE = 32\n",
    "REPLAY_MEMORY_SIZE = 1000000  # Number of recent frames used for optimization updates during training\n",
    "AGENT_HISTORY_LENGTH = 4\n",
    "TARGET_NETWORK_UPDATE_FREQUENCY = 10000  \n",
    "DISCOUNT_FACTOR = 0.99\n",
    "ACTION_REPEAT = 4  # Frame skip: Number of times each action is repeated (results in agent seeing only every 4th frame)\n",
    "UPDATE_FREQUENCY = 4  # Number of actions selected by agent between successive SGD updates\n",
    "LEARNING_RATE = 0.0000625  # DQN Paper uses 0.00025, Rainbow paper uses 0.0000625, people online use 0.00001\n",
    "GRADIENT_MOMENTUM = 0.95  # Squared gradient momentum same\n",
    "MIN_SQUARED_GRADIENT = 0.01\n",
    "INITIAL_EXPLORATION = 1\n",
    "FINAL_EXPLORATION = 0.1\n",
    "FINAL_EXPLORATION_FRAME = 1000000 / ACTION_REPEAT  # Exploration rate decayed over 1 million frames where each frame has 4 frame skips\n",
    "REPLAY_START_SIZE = 50000\n",
    "NOOP_MAX = 30  # Max number of \"do nothing\" actions to be performed by agent at start of an episode\n",
    "# optimizer = tf.optimizers.RMSprop(learning_rate=LEARNING_RATE, rho=GRADIENT_MOMENTUM, epsilon=MIN_SQUARED_GRADIENT) \n",
    "optimizer = tf.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "NOOP_ACTION = 0  # Atari Breakout actions: 0 (noop), 1 (fire), 2 (left) and 3 (right)\n",
    "PROBLEM = 'BreakoutDeterministic-v4'  # Deterministic: fixed frame skip of 4 (ACTION_REPEAT), v4: 0 repeat action probability\n",
    "NUMBER_OF_EPISODES = 1000000\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS = 84, 84, 1\n",
    "CONV1_NUM_FILTERS, CONV1_FILTER_SIZE, CONV1_FILTER_STRIDES = 32, 8, 4\n",
    "CONV2_NUM_FILTERS, CONV2_FILTER_SIZE, CONV2_FILTER_STRIDES = 64, 4, 2\n",
    "CONV3_NUM_FILTERS, CONV3_FILTER_SIZE, CONV3_FILTER_STRIDES = 64, 3, 1\n",
    "CONV3_OUT_DIM_VALID_PADDING, CONV3_OUT_DIM_SAME_PADDING, PADDING = 7, 11, \"VALID\"\n",
    "DENSE_NUM_UNITS, OUTPUT_NUM_UNITS = 512, 4\n",
    "WEIGHT_INITIALIZER = tf.initializers.VarianceScaling(scale=2.0)  # Scale 2.0 for RELU activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_OrJcWQBicL-"
   },
   "source": [
    "### Frame Preprocessor\n",
    "Responsible for preprocessing raw RGB pixel values in Atari video frames into downscaled, cropped and normalized greyscale images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ox6Z59jpjl_u"
   },
   "outputs": [],
   "source": [
    "class FramePreprocessor:\n",
    "    \"\"\"\n",
    "    FramePreprocessor re-sizes, normalizes and converts RGB atari frames to gray scale frames.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_space):\n",
    "        self.state_space = state_space\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_rgb_to_grayscale(tf_frame):\n",
    "        return tf.image.rgb_to_grayscale(tf_frame)\n",
    "\n",
    "    @staticmethod\n",
    "    def resize_frame(tf_frame, frame_height, frame_width):  # TODO: crop down to bounding box of playing area\n",
    "        return tf.image.resize(tf_frame, [frame_height, frame_width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_frame_from_greyscale_values(image):\n",
    "        height, width, _ = image.shape # stacked_image = np.stack((img,)*3, axis=-1)\n",
    "        grey_image = np.array([[(image[i, j].numpy()[0], image[i, j].numpy()[0], image[i, j].numpy()[0])\n",
    "                               for i in range(height)]\n",
    "                               for j in range(width)])\n",
    "        grey_image = np.transpose(grey_image, (1, 0, 2))  # Switch height and width\n",
    "        plt.imshow(grey_image)\n",
    "        plt.show()\n",
    "\n",
    "    def preprocess_frame(self, frame):\n",
    "        tf_frame = tf.Variable(frame, shape=self.state_space, dtype=tf.uint8)\n",
    "        image = self.convert_rgb_to_grayscale(tf_frame)\n",
    "        image = self.resize_frame(image, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "        image = tf.reshape(image, shape=(IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s1duVCRjispx"
   },
   "source": [
    "### Replay Memory\n",
    "\n",
    "Memory data structure used to store experiences of agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rpUHMS_T6tzj"
   },
   "outputs": [],
   "source": [
    "# Not currently used to encode experience: (state, action, reward, next_state, is_done)\n",
    "from typing import NamedTuple, Tuple \n",
    "class Experience(NamedTuple): \n",
    "  state: Tuple[int, int, int] \n",
    "  action: int \n",
    "  reward: float \n",
    "  next_state: Tuple[int, int, int]\n",
    "  is_done: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VPZ3K5_Njqxr"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    Memory class holds a list of game plays stored as experiences (s,a,r,s', d) = (state, action, reward, next_state, is_done)\n",
    "    Credits: https://stackoverflow.com/questions/40181284/how-to-get-random-sample-from-deque-in-python-3 \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):  # Initialize memory with given capacity\n",
    "        self.experiences = [None] * capacity\n",
    "        self.capacity = capacity\n",
    "        self.index = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def add(self, experience):  # Add a sample to the memory, removing the earliest entry if memeory capacity is reached\n",
    "        self.experiences[self.index] = experience\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "        self.index = (self.index + 1) % self.capacity  # Overwrites earliest entry if memory capacity reached\n",
    "\n",
    "    def sample(self, size):\n",
    "        indices = random.sample(range(self.size), size)\n",
    "        return [self.experiences[index] for index in indices]  # Efficient random access\n",
    "    \n",
    "    def extract_memory(self):\n",
    "        if self.experiences[0] == None:\n",
    "            return np.array([])\n",
    "        return self.sample(REPLAY_START_SIZE+1)\n",
    "\n",
    "    def load_memory(self,replay_memory): \n",
    "        if replay_memory == []: return\n",
    "        [self.add(experience) for experience in replay_memory if experience != None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "srHfxJqwixmx"
   },
   "source": [
    "### Model\n",
    "Convolutional Neural Network used to approximate Q-values from raw Atari video pixels \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "odVTPuhrnClV"
   },
   "source": [
    "#### Parameters\n",
    "Weights and biases of primary and target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f8OR3yOahCBc"
   },
   "outputs": [],
   "source": [
    "weights = {  # 4D: Filter Width, Height, In Channel, Out Channel\n",
    "    # Conv Layer 1: 8x8 conv, 1 input (preprocessed stacked image has 4 channels), 32 output filters\n",
    "    'conv1_weights': tf.Variable(WEIGHT_INITIALIZER([CONV1_FILTER_SIZE, CONV1_FILTER_SIZE, AGENT_HISTORY_LENGTH, CONV1_NUM_FILTERS])),\n",
    "    # Conv Layer 2: 4x4 conv, 32 input filters, 64 output filters\n",
    "    'conv2_weights': tf.Variable(WEIGHT_INITIALIZER([CONV2_FILTER_SIZE, CONV2_FILTER_SIZE, CONV1_NUM_FILTERS, CONV2_NUM_FILTERS])),\n",
    "    # Conv Layer 3: 3x3 conv, 64 input filters, 64 output filters\n",
    "    'conv3_weights': tf.Variable(WEIGHT_INITIALIZER([CONV3_FILTER_SIZE, CONV3_FILTER_SIZE, CONV2_NUM_FILTERS, CONV3_NUM_FILTERS])),\n",
    "    # Fully Connected (Dense) Layer: 7x7x64 inputs (64 filters of size 3x3), 512 output units\n",
    "    'dense_weights': tf.Variable(WEIGHT_INITIALIZER([CONV3_OUT_DIM_VALID_PADDING * CONV3_OUT_DIM_VALID_PADDING * CONV3_NUM_FILTERS, DENSE_NUM_UNITS])),\n",
    "    # Output layer: 512 input units, 4 output units (actions)\n",
    "    'output_weights': tf.Variable(WEIGHT_INITIALIZER([DENSE_NUM_UNITS, OUTPUT_NUM_UNITS]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'conv1_biases': tf.Variable(tf.zeros([CONV1_NUM_FILTERS])),  # 32\n",
    "    'conv2_biases': tf.Variable(tf.zeros([CONV2_NUM_FILTERS])),  # 64\n",
    "    'conv3_biases': tf.Variable(tf.zeros([CONV3_NUM_FILTERS])),  # 64\n",
    "    'dense_biases': tf.Variable(tf.zeros([DENSE_NUM_UNITS])),  # 512\n",
    "    'output_biases': tf.Variable(tf.zeros([OUTPUT_NUM_UNITS]))  # 4\n",
    "}\n",
    "\n",
    "target_weights = {\n",
    "    'conv1_target_weights': tf.Variable(WEIGHT_INITIALIZER([CONV1_FILTER_SIZE, CONV1_FILTER_SIZE, AGENT_HISTORY_LENGTH, CONV1_NUM_FILTERS])),\n",
    "    'conv2_target_weights': tf.Variable(WEIGHT_INITIALIZER([CONV2_FILTER_SIZE, CONV2_FILTER_SIZE, CONV1_NUM_FILTERS, CONV2_NUM_FILTERS])),\n",
    "    'conv3_target_weights': tf.Variable(WEIGHT_INITIALIZER([CONV3_FILTER_SIZE, CONV3_FILTER_SIZE, CONV2_NUM_FILTERS, CONV3_NUM_FILTERS])),\n",
    "    'dense_target_weights': tf.Variable(WEIGHT_INITIALIZER([CONV3_OUT_DIM_VALID_PADDING * CONV3_OUT_DIM_VALID_PADDING * CONV3_NUM_FILTERS, DENSE_NUM_UNITS])),\n",
    "    'output_target_weights': tf.Variable(WEIGHT_INITIALIZER([DENSE_NUM_UNITS, OUTPUT_NUM_UNITS]))\n",
    "}\n",
    "\n",
    "target_biases = {\n",
    "    'conv1_target_biases': tf.Variable(tf.zeros([CONV1_NUM_FILTERS])),  # 32\n",
    "    'conv2_target_biases': tf.Variable(tf.zeros([CONV2_NUM_FILTERS])),  # 64\n",
    "    'conv3_target_biases': tf.Variable(tf.zeros([CONV3_NUM_FILTERS])),  # 64\n",
    "    'dense_target_biases': tf.Variable(tf.zeros([DENSE_NUM_UNITS])),  # 512\n",
    "    'output_target_biases': tf.Variable(tf.zeros([OUTPUT_NUM_UNITS]))  # 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nmaJjfQYnJQD"
   },
   "source": [
    "#### Convolutional Neural Network \n",
    "Responsible for predicting Q-values of states given actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Y9eiJ3OjtFl"
   },
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork:\n",
    "    \"\"\"\n",
    "    CNN Architecture of DQN has 1 input layer, 4 hidden layers (3 convolutions and 1 dense after flatten) and 1 output layer:\n",
    "    Input:  84 X 84 X 4 image (4 due to image stacking) \n",
    "    1st Hidden layer: Convolves 32 filters of 8 X 8 with stride 4 (relu)\n",
    "    2nd hidden layer: Convolves 64 filters of 4 X 4 with stride 2 (relu)\n",
    "    3rd hidden layer: Convolves 64 filters of 3 X 3 with stride 1 (Relu)\n",
    "    4th hidden layer: Fully connected, (512 relu units)\n",
    "    Output: Fully connected linear layer, Separate output unit for each action, outputs are predicted Q-values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, number_of_states, number_of_actions):\n",
    "        self.weights = weights \n",
    "        self.biases = biases \n",
    "        self.target_weights = target_weights\n",
    "        self.target_biases = target_biases \n",
    "        self.number_of_states = number_of_states\n",
    "        self.number_of_actions = number_of_actions\n",
    "\n",
    "    @tf.function\n",
    "    def normalize_images(self, images):\n",
    "        return tf.cast(images / 255, dtype=tf.float32)\n",
    "\n",
    "    @tf.function\n",
    "    def convolutional_2d_layer(self, inputs, filter_weights, biases, strides=1):\n",
    "        output = tf.nn.conv2d(inputs, filter_weights, strides, padding=PADDING)\n",
    "        output_with_bias = tf.nn.bias_add(output, biases)\n",
    "        activation = tf.nn.leaky_relu(output_with_bias)  # non-linearity \n",
    "        return activation\n",
    "\n",
    "    @tf.function\n",
    "    def flatten_layer(self, layer):  # output shape: [32, 64*84*84]\n",
    "        # Shape: Minibatches: 32, Num of Filters * Input Height (conv3), Input Width (conv3): 64*7*7 = 3136\n",
    "        memory_batch_size, image_height, image_width, num_filters = layer.get_shape()\n",
    "        flattened_layer = tf.reshape(layer, (memory_batch_size, num_filters * image_height * image_width))\n",
    "        return flattened_layer\n",
    "\n",
    "    @tf.function\n",
    "    def dense_layer(self, inputs, weights, biases):\n",
    "        output = tf.nn.bias_add(tf.matmul(inputs, weights), biases)\n",
    "        dense_activation = tf.nn.leaky_relu(output)  # non-linearity\n",
    "        return dense_activation\n",
    "\n",
    "    @tf.function\n",
    "    def output_layer(self, input, weights, biases):\n",
    "        linear_output = tf.nn.bias_add(tf.matmul(input, weights), biases)\n",
    "        return linear_output\n",
    "\n",
    "    @tf.function\n",
    "    def huber_error_loss(self, y_true, y_predictions, delta=1.0):\n",
    "        y_predictions = tf.cast(y_predictions, dtype=tf.float32)\n",
    "        errors = y_true - y_predictions\n",
    "        condition = tf.abs(errors) <= delta\n",
    "        l2_squared_loss = 0.5 * tf.square(errors)\n",
    "        l1_absolute_loss = delta * (tf.abs(errors) - 0.5 * delta)\n",
    "        loss = tf.where(condition, l2_squared_loss, l1_absolute_loss)\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def train(self, inputs, targets):  # Optimization\n",
    "        # Wrap computation inside a GradientTape for automatic differentiation\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.predict(inputs)  # Q(s,a)\n",
    "            current_loss = self.huber_error_loss(predictions, targets)\n",
    "\n",
    "        # Trainable variables to update\n",
    "        trainable_variables = list(self.weights.values()) + list(self.biases.values())\n",
    "\n",
    "        gradients = tape.gradient(current_loss, trainable_variables)\n",
    "\n",
    "        # Update weights and biases following gradients\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        return tf.reduce_mean(current_loss)\n",
    "\n",
    "    @tf.function\n",
    "    def predict(self, inputs, is_target=False):  # 4D input for CNN: (batch_size, height, width, depth)\n",
    "        # Input shape: [32 or 1, 84, 84, 4]. A batch of 84x84x4 stacked (gray scale) images.\n",
    "        inputs = self.normalize_images(inputs)\n",
    "\n",
    "        # Convolution Layer 1 with output shape [32 or 1, 20, 20, 32]\n",
    "        conv1_weights = self.target_weights['conv1_target_weights'] if is_target else self.weights['conv1_weights']\n",
    "        conv1_biases = self.target_biases['conv1_target_biases'] if is_target else self.biases['conv1_biases']\n",
    "        conv1 = self.convolutional_2d_layer(inputs, conv1_weights, conv1_biases, strides=CONV1_FILTER_STRIDES)\n",
    "\n",
    "        # Convolutional Layer 2 with output shape [32 or 1, 9, 9, 64]\n",
    "        conv2_weights = self.target_weights['conv2_target_weights'] if is_target else self.weights['conv2_weights']\n",
    "        conv2_biases = self.target_biases['conv2_target_biases'] if is_target else self.biases['conv2_biases']\n",
    "        conv2 = self.convolutional_2d_layer(conv1, conv2_weights, conv2_biases, strides=CONV2_FILTER_STRIDES)\n",
    "\n",
    "        # Convolutional Layer 3 with output shape [32 or 1, 7, 7, 64]\n",
    "        conv3_weights = self.target_weights['conv3_target_weights'] if is_target else self.weights['conv3_weights']\n",
    "        conv3_biases = self.target_biases['conv3_target_biases'] if is_target else self.biases['conv3_biases']\n",
    "        conv3 = self.convolutional_2d_layer(conv2, conv3_weights, conv3_biases, strides=CONV3_FILTER_STRIDES)\n",
    "\n",
    "        # Flatten output of 3nd conv. layer to fit dense layer input, output shape [32 or 1, 64*7*7]\n",
    "        flattened_layer = self.flatten_layer(layer=conv3)\n",
    "\n",
    "        # Dense fully connected layer with output shape [32 or 1, 512]\n",
    "        dense_weights = self.target_weights['dense_target_weights'] if is_target else self.weights['dense_weights']\n",
    "        dense_biases = self.target_biases['dense_target_biases'] if is_target else self.biases['dense_biases']\n",
    "        dense_layer = self.dense_layer(flattened_layer, dense_weights, dense_biases)\n",
    "\n",
    "        # Fully connected output of shape [32 or 1, 4]\n",
    "        output_weights = self.target_weights['output_target_weights'] if is_target else self.weights['output_weights']\n",
    "        output_biases = self.target_biases['output_target_biases'] if is_target else self.biases['output_biases']\n",
    "        output_layer = self.output_layer(dense_layer, output_weights, output_biases)\n",
    "\n",
    "        return output_layer\n",
    "\n",
    "    @tf.function\n",
    "    def overwrite_model_params(self):  # Assume same order and length\n",
    "        for weight, target_weight_key in zip(self.weights.values(), self.target_weights.keys()):\n",
    "            self.target_weights[target_weight_key].assign(tf.identity(weight))\n",
    "\n",
    "        for bias, target_bias_key in zip(self.biases.values(), self.target_biases.keys()):\n",
    "            self.target_biases[target_bias_key].assign(tf.identity(bias))\n",
    "\n",
    "    @tf.function\n",
    "    def load_tf_variables(self, model_parameters, raw_parameters, title=None):\n",
    "        for key in raw_parameters:\n",
    "            model_parameters[key].assign(tf.identity(raw_parameters[key]))\n",
    "            print(f'{title} - Loaded: {key}')\n",
    "        if title is not None: \n",
    "            print(f'Done Loading: {title}')\n",
    "\n",
    "    def load_model_parameters(self, raw_parameters):\n",
    "        self.load_tf_variables(self.biases, raw_parameters['biases'], 'Biases')\n",
    "        self.load_tf_variables(self.weights, raw_parameters['weights'], 'Weights')\n",
    "        self.overwrite_model_params()  # Copy loaded parameters to target network\n",
    "        print('Parameters loaded')\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_tf_parameters_to_native(parameters, title=None):\n",
    "        extracted_params = {}\n",
    "        for key in parameters:\n",
    "            extracted_params[key] = parameters[key].numpy().tolist()\n",
    "        if title is not None: print(f'Extracted: {title} - {parameters[key].shape}')\n",
    "        return extracted_params\n",
    "\n",
    "    def extract_parameters(self):\n",
    "        parameters = {\n",
    "            'weights': self.convert_tf_parameters_to_native(self.weights, 'weights'),\n",
    "            'biases': self.convert_tf_parameters_to_native(self.biases, 'biases')\n",
    "        }\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VQ1HmPVki53k"
   },
   "source": [
    "### Agent\n",
    "Deep Q Network Agent uses Deep Convolutional Network to approximate Q-values of states given actions and Q Reinforcement Learning to learn an optimal policy in the Breakout Atari Open AI game (i.e. gym) environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVsOAC7jjwuy"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Agent takes actions and saves them to its memory, which is initialized with a given capacity\n",
    "    \"\"\"\n",
    "    steps = 0\n",
    "    exploration_rate = INITIAL_EXPLORATION\n",
    "\n",
    "    def decay_exploration_rate(self):\n",
    "        decay_rate = (self.exploration_rate - FINAL_EXPLORATION) / FINAL_EXPLORATION_FRAME\n",
    "        return decay_rate\n",
    "\n",
    "    # Initialize agent with a given memory capacity, and a state, and action space\n",
    "    def __init__(self, number_of_states, number_of_actions):\n",
    "        self.experiences = ReplayMemory(REPLAY_MEMORY_SIZE)\n",
    "        self.model = ConvolutionalNeuralNetwork(number_of_states, number_of_actions)  \n",
    "        self.number_of_states = number_of_states\n",
    "        self.number_of_actions = number_of_actions\n",
    "        self.decay_rate = self.decay_exploration_rate()\n",
    "\n",
    "    # The behaviour policy during training was e-greedy with e annealed linearly\n",
    "    # from 1.0 to 0.1 over the first million frames, and fixed at 0.1 thereafter\n",
    "    def e_greedy_policy(self, state):\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > self.exploration_rate:\n",
    "            next_q_values = self.model.predict(state)\n",
    "            best_action = np.argmax(next_q_values)  \n",
    "        else:\n",
    "            best_action = self.random_policy()\n",
    "        return best_action\n",
    "\n",
    "    def random_policy(self):\n",
    "        return random.randint(0, self.number_of_actions - 1)\n",
    "\n",
    "    def act(self, state):\n",
    "        return self.random_policy() if self.experiences.size <= REPLAY_START_SIZE else self.e_greedy_policy(state)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.model.overwrite_model_params()\n",
    "\n",
    "    def observe(self, experience):\n",
    "        self.experiences.add(experience)\n",
    "        self.steps += 1\n",
    "        self.exploration_rate = (FINAL_EXPLORATION if self.exploration_rate <= FINAL_EXPLORATION \n",
    "                                 else self.exploration_rate - self.decay_rate)\n",
    "\n",
    "        if self.steps % TARGET_NETWORK_UPDATE_FREQUENCY == 0:\n",
    "            self.update_target_model()\n",
    "\n",
    "    def replay(self):  # Experience: (state, action, reward, next_state, is_done) # Train neural net with experiences\n",
    "        memory_batch = self.experiences.sample(MINIBATCH_SIZE)\n",
    "        states = tf.reshape([state for (state, *rest) in memory_batch],\n",
    "                            shape=(MINIBATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDTH, AGENT_HISTORY_LENGTH))\n",
    "        next_states = tf.reshape([next_state for (_, _, _, next_state, _) in memory_batch],\n",
    "                                 shape=(MINIBATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDTH, AGENT_HISTORY_LENGTH))\n",
    "\n",
    "        current_q_predictions = self.model.predict(states)\n",
    "        next_q_predictions_target = self.model.predict(next_states, is_target=True)\n",
    "\n",
    "        targets = np.zeros(shape=(MINIBATCH_SIZE, self.number_of_actions))\n",
    "        for i, (state, action, current_reward, next_state, is_done) in enumerate(memory_batch):\n",
    "            current_q_values = current_q_predictions[i].numpy()  # Q(s,a) ~= Q(s,a,theta)\n",
    "            next_q_values_target = tf.reshape(next_q_predictions_target[i], shape=[-1])  # Q(s', a') ~ Q(s', a', target)\n",
    "\n",
    "            future_discounted_reward = DISCOUNT_FACTOR * tf.reduce_max(next_q_values_target)  # max_a' Q(s', a', target)\n",
    "            current_q_values[action] = current_reward if is_done else current_reward + future_discounted_reward \n",
    "            targets[i] = current_q_values\n",
    "        targets = tf.convert_to_tensor(targets)\n",
    "        average_loss = self.model.train(states, targets)  # Q(s,a) predictions, Q(s',a',theta_target) targets\n",
    "        return average_loss\n",
    "\n",
    "    def extract_model(self):\n",
    "        return self.model.extract_parameters()\n",
    "    \n",
    "    def extract_replay_memory(self):\n",
    "        return self.experiences.extract_memory()\n",
    "\n",
    "    def load_replay_memory(self,replay_memory):\n",
    "        self.experiences.load_memory(replay_memory)\n",
    "\n",
    "    def extract_agent_parameters(self):\n",
    "        return {'exploration_rate': self.exploration_rate, 'steps': self.steps}\n",
    "\n",
    "    def load_agent_parameters(self, parameters):\n",
    "        self.exploration_rate = parameters['exploration_rate']\n",
    "        self.steps = parameters['steps']\n",
    "\n",
    "    def load_model(self, raw_parameters):\n",
    "        self.model.load_model_parameters(raw_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uDqW7Cm3jpSP"
   },
   "source": [
    "### Environment\n",
    "Environment used to wrap Atari Learning Environment (ALE) from Open Gym library with customized step and reset state functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lGLRv7eykNx3"
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"\n",
    "    Creates a game environment which an agent can play using certain actions.\n",
    "    Run takes an agent as argument that plays the game, until the agent 'dies' (no more lives)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, problem, video_utility):\n",
    "        #self.gym = video_utility.wrap_environment(gym.make(problem))\n",
    "        self.gym = gym.make(problem)\n",
    "        self.video_utility = video_utility\n",
    "        self.state_space = (IMAGE_HEIGHT, IMAGE_WIDTH, AGENT_HISTORY_LENGTH)  # Image dimensions before preprocessing\n",
    "        self.frame_preprocessor = FramePreprocessor(self.gym.observation_space.shape)\n",
    "        self.best_reward, self.acc_reward, self.life_count, self.steps = 0, 0, 0, 0\n",
    "        self.state, self.next_state = [], []\n",
    "\n",
    "    def extract_environment_parameters(self):\n",
    "        return {'best_reward': self.best_reward, 'acc_reward': self.acc_reward}\n",
    "\n",
    "    def load_environment_parameters(self, parameters):\n",
    "        self.best_reward = parameters['best_reward']\n",
    "        self.acc_reward = parameters['acc_reward']\n",
    "\n",
    "    def get_num_of_actions(self):\n",
    "        return self.gym.action_space.n\n",
    "\n",
    "    def get_num_of_states(self):\n",
    "        return self.state_space\n",
    "\n",
    "    @tf.function  # Clip rewards to 1 if positive and -1 if negative\n",
    "    def clip_reward(self, reward):\n",
    "        return tf.sign(reward)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_frame(frame, state):\n",
    "        if len(state) >= AGENT_HISTORY_LENGTH:\n",
    "            state.pop()\n",
    "        state.append(frame)\n",
    "\n",
    "    def add_frame_to_state(self, frame, is_next_frame=False):\n",
    "        self.add_frame(frame, self.next_state) if is_next_frame else self.add_frame(frame, self.state)\n",
    "\n",
    "    def generate_random_state_from_n_frames(self, agent, current_frame, num_of_frames=AGENT_HISTORY_LENGTH):\n",
    "        while len(self.state) < num_of_frames:\n",
    "            self.add_frame_to_state(current_frame)\n",
    "            action = agent.random_policy()\n",
    "            next_frame, reward, is_done, info = self.gym.step(action)\n",
    "            next_frame = self.frame_preprocessor.preprocess_frame(next_frame)\n",
    "            self.add_frame_to_state(next_frame, is_next_frame=True)\n",
    "            current_frame = next_frame\n",
    "            self.steps += 1\n",
    "\n",
    "    @staticmethod\n",
    "    def reshape_state(state, batch_size=1):\n",
    "        state = tf.stack(state, axis=-1)  # stack 4 frames into 4 channels\n",
    "        return tf.reshape(state, shape=(batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, AGENT_HISTORY_LENGTH))\n",
    "\n",
    "    def reset(self):\n",
    "        self.life_count, self.steps = 0, 0\n",
    "        start_state = self.gym.reset()\n",
    "        return self.frame_preprocessor.preprocess_frame(start_state)\n",
    "\n",
    "    def get_terminal(self, info, is_done):\n",
    "        new_life_count = info['ale.lives']\n",
    "        lost_life = True if new_life_count < self.life_count else False\n",
    "        self.life_count = new_life_count\n",
    "        terminal = True if is_done or lost_life else is_done\n",
    "        return terminal\n",
    "\n",
    "    def step(self, agent):  # (Action, State) : New State\n",
    "        self.state = self.reshape_state(self.state)\n",
    "        action = agent.act(self.state)\n",
    "        next_frame, reward, is_done, info = self.gym.step(action)\n",
    "        next_frame = self.frame_preprocessor.preprocess_frame(next_frame)\n",
    "        self.add_frame_to_state(next_frame, is_next_frame=True)\n",
    "        # reward = self.clip_reward(reward) # Generalization to other Atari games \n",
    "        is_done = self.get_terminal(info, is_done)\n",
    "        #if is_done:\n",
    "            #self.gym.stats_recorder.save_complete()\n",
    "            #self.gym.stats_recorder.done = True\n",
    "        next_state = tf.zeros(shape=(1, IMAGE_WIDTH, IMAGE_HEIGHT, AGENT_HISTORY_LENGTH),\n",
    "                              dtype=tf.uint8) if is_done else self.next_state\n",
    "        return self.state, action, reward, next_state, is_done\n",
    "\n",
    "    def run(self, agent, should_print_save=False, is_train=False):\n",
    "        total_reward = 0\n",
    "        self.life_count = 0\n",
    "        average_train_losses = []\n",
    "        current_frame = self.reset()\n",
    "        self.generate_random_state_from_n_frames(agent, current_frame)\n",
    "        [self.gym.step(NOOP_ACTION) for _ in range(random.randint(0, NOOP_MAX))]  # No action random number of times\n",
    "\n",
    "        while True:\n",
    "            if should_print_save:\n",
    "                self.gym.render()\n",
    "\n",
    "            self.state, action, reward, next_state, is_done = self.step(agent)\n",
    "            experience = self.reshape_state(self.state), action, reward, self.reshape_state(next_state), is_done\n",
    "            agent.observe(experience)\n",
    "\n",
    "            if agent.experiences.size >= REPLAY_START_SIZE and is_train:\n",
    "                if self.steps % UPDATE_FREQUENCY == 0:  # Only train every 4th step (action)\n",
    "                    average_train_loss = agent.replay()\n",
    "                    average_train_losses.append(average_train_loss.numpy())\n",
    "\n",
    "            self.state = self.next_state\n",
    "            self.steps += 1\n",
    "            total_reward += reward\n",
    "\n",
    "            if is_done:  \n",
    "                if is_train or agent.experiences.size  >= REPLAY_START_SIZE:\n",
    "                    break # Break when memory filled or training is done \n",
    "                current_frame = self.reset() # Initially play randomly \n",
    "\n",
    "        if is_train:\n",
    "            self.best_reward = total_reward if total_reward > self.best_reward else self.best_reward\n",
    "            self.acc_reward += total_reward\n",
    "            average_episode_loss = float(np.mean(average_train_losses)) if len(average_train_losses) > 0 else 0.0\n",
    "            return average_episode_loss, total_reward\n",
    "        else:\n",
    "            total_reward = 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RQXP_a8XjwMn"
   },
   "source": [
    "### Main Method\n",
    "Setup model name to be stored in \"aml_models\" folder in Colab Notebooks folder in Google Drive if IS_GOOGLE is toggled True in top of file. Otherwise, the model is automatically stored locally. Remember to set a model name in \"MODEL_NAME\" and set the interval of how often the periodic results of the DQN agent should be printed and the models stored down below. The memory of the agent is initally pre-populated with random play experiences before training episodes are started using the Convolutional Neural Network and Q-Learning  during Agent replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "l4t5pDmhlFpT",
    "outputId": "739dd033-944d-4286-ec70-31541e46e19a"
   },
   "outputs": [],
   "source": [
    "# Setup learning environment\n",
    "MODEL_NAME = f\"stored_model_original_Adam_He_Leaky_ReLU\"  \n",
    "PRINT_SAVE_INTERVAL = 100  # NB: do not set low save interval since gzip is not atomic\n",
    "filehandler = FileHandler(filename=MODEL_NAME)\n",
    "video_utility = VideoUtility(MODEL_NAME)\n",
    "environment = Environment(PROBLEM, video_utility)\n",
    "dqn_agent = Agent(environment.get_num_of_states(), environment.get_num_of_actions())\n",
    "\n",
    "\n",
    "# Load session \n",
    "timer = Timer(\"Total Session\")\n",
    "initial_episode, average_losses, total_rewards, average_rewards = filehandler.load_session(dqn_agent, environment)\n",
    "episodes = [] if initial_episode == 0 else list(range(1, initial_episode + 1))\n",
    "labels = [(average_losses, f\"Average loss over time ({MODEL_NAME})\", \"Average loss\"),\n",
    "          (total_rewards, f\"Reward over time ({MODEL_NAME})\", \"Reward\"),\n",
    "          (average_rewards, f\"Average reward over time ({MODEL_NAME})\", \"Average reward\")]\n",
    "\n",
    "# Pre-populate memory with random experiences \n",
    "if dqn_agent.experiences.size <= REPLAY_START_SIZE:\n",
    "    print('Filling memory')\n",
    "    environment.run(dqn_agent, should_print_save=False, is_train=False)  \n",
    "    clear_output()\n",
    "\n",
    "print('Start training')\n",
    "# Start learning experiemnts \n",
    "for episode in range(initial_episode + 1, NUMBER_OF_EPISODES):  # Start train\n",
    "    SHOULD_PRINT_SAVE = episode % PRINT_SAVE_INTERVAL == 0 and dqn_agent.experiences.size >= REPLAY_START_SIZE\n",
    "    average_loss, total_reward = environment.run(dqn_agent, SHOULD_PRINT_SAVE, is_train=True)\n",
    "    average_reward = np.round(environment.acc_reward / (episode), 4)\n",
    "    episodes.append(episode)\n",
    "    average_losses.append(average_loss)\n",
    "    total_rewards.append(total_reward)\n",
    "    average_rewards.append(average_reward)  \n",
    "\n",
    "    if SHOULD_PRINT_SAVE:\n",
    "        video_utility.clear_files()\n",
    "        clear_output()\n",
    "        print(f\"Episode: {episode} - Overall best reward: {environment.best_reward} - \"\n",
    "              f\"Avg reward: {average_reward} - Learning Rate: {dqn_agent.exploration_rate}\")\n",
    "        timer.stop()\n",
    "        [plot(episodes, y, title, ylabel) for y, title, ylabel in labels]\n",
    "        #video_utility.show_video()\n",
    "        filehandler.save_session(dqn_agent, environment, episode, average_losses, total_rewards, average_rewards)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Submission - Final_Deep_Reinforcement_Learning_Agent_Atari.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
